<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Gopala Anumanchipalli - Berkeley Speech Group</title>

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

<style>
body {
 font-family:"Helvetica Neue","Helvetica","Arial",sans-serif;
 -webkit-font-smoothing:antialiased;
 background-color : #F4F5F6;
}
h3, h4, h5 {
  font-weight : 500;
  letter-spacing : .01em;
}
.publogo { margin-right : 20px; }
.publogo-soon { background: rgb(157,157,157);
background: linear-gradient(0deg, rgba(157,157,157,1) 0%, rgba(170,170,170,1) 100%);}
.publication { padding-left : 0px; padding-top : 3px; margin-bottom : 0px;}
.publication strong a { color : #000; text-decoration  :none; }
.publication strong a:hover { text-decoration : underline; }
.publication .links a { margin-right : 15px; }
ul{ list-style:none; padding:0; margin:0; }
ul ul { padding-top : 6px; list-style : disc; padding-left : 40px; }
ul li { padding-bottom:6px; }
.nameplate {
  display: flex;
  flex-direction: column;
  justify-content: center;
}
a, a:hover { color : #3B7EA1; }
nav a, nav a:hover { color : #000; }
.student { text-align : left; align-self: center;}
.student a, .student a:hover { color : inherit; }

.topbar { 
background: #003262;
color : #fff;  
}
.topbar a { color : #fff; }
.topbar h3 { color : #fff; }
.darker { background-color : #E9EBED;
}
h4 { margin-top : 30px; }

h5 { display : flex }
.papers-selected .paperlo { display : none; }
.papers-selected h5 { display : none; }
.papers-selected .publication {
  display : none; 
}
.paperhi-only { display : none; }
.papers-selected .paperhi, .papers-selected .paperhi-only {
  display : flex; 
}

.btn-dark { display : none; }
.btn-light { display : inline; }

</style>

<link rel="shortcut icon" href="./Berkeley Speech Group_Files/eecs_logo.jpg">

</head>
<body data-new-gr-c-s-check-loaded="14.1080.0" data-gr-ext-installed="">

<script type="text/javascript" src="./Berkeley Speech Group_Files/script.js"></script>

<div class="topbar">
<div class="container">
<div>

  <div class="row mt-0">
  <div class="col-6 col-sm-6 col-md-4 col-lg-3 pl-4 py-4 ml-md-auto"><a href="./Berkeley Speech Group_Files/gopala.jpg"><img alt="" src="./Berkeley Speech Group_Files/gopala-small.jpg" id="me" class="float-left img-fluid img-fluid shadow-sm"></a></div>
 

      <div class="col-12 col-sm-12 col-md-7 col-lg-5  pl-4 pb-4 py-0 py-md-1 mr-auto nameplate">
        <h3>Gopala Anumanchipalli</h3>
          <p class="my-0">
              Assistant Professor<br>
              Department of Electrical Engineering and Computer Sciences<br>            
              University of California, Berkeley <br>
             <br>
             Office: 490A Cory Hall<br>
             Email: gopala at eecs dot berkeley dot edu<br><br>
             <a href="https://scholar.google.com/citations?user=VecEj6kAAAAJ&hl=en">Google Scholar</a> |
             <a href="https://github.com/Berkeley-Speech-Group">Github</a> |
             <a href="https://twitter.com/GopalaSpeech">Twitter</a> |
             <a href="./Berkeley Speech Group_Files/anumanchipalli_cv.pdf">CV</a>
          </p>    
     </div>

  </div>

</div>
</div>
</div>

<div class="container">
<div>

  <div class="row pb-4">

    <div class="col-lg-6 order-0 col-12 px-4 papers-container papers-selected">
      <h4 class="paperhi">Bio</h4>
            <p>I am an Assistant Professor in the Department of Electrical Engineering and Computer Sciences at <a href="https://www.berkeley.edu/">UC Berkeley</a>, and in the Department of Neurosurgery at <a href="https://www.ucsf.edu/">UC San Francisco</a>, where I lead the Berkeley Speech Group.</p>
            <p>I did my PhD at <a href="https://www.cmu.edu/">Carnegie Mellon University</a> and <a href="https://tecnico.ulisboa.pt/en/">Instituto Superior Tecnico</a>, where I was advised by <a href="http://www.cs.cmu.edu/~awb/">Alan Black</a> and <a href="https://www.hlt.inesc-id.pt/~lco/">Luis Oliveira</a>, and was a postdoc at <a href="https://www.ucsf.edu/">UC San Francisco</a> with <a href="https://profiles.ucsf.edu/edward.chang">Edward Chang</a>. Earlier, I got my B.Tech and M.S. degrees from <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a> under <a href="https://en.wikipedia.org/wiki/Raj_Reddy">Raj Reddy</a>.</p>
    </div>

    <div class="col-lg-6 order-0 col-12 px-4 papers-container papers-selected">

      <h4 class="paperhi">Research</h4>
      <p>Our group studies the intersection of speech processing, neuroscience, and artificial intelligence with an emphasis on human-centred speech and assistive technologies, including new paradigms for bio-inspired spoken language technologies, automated methods for early diagnosis, characterizing and rehabilitating disordered speech.</p>

      <p>Our group is a part of <a href="https://bair.berkeley.edu/">Berkeley AI Research (BAIR)</a>. Prospective PhD students should apply <a href="https://eecs.berkeley.edu/academics/graduate/research-programs/admissions">here</a>.</p> 

    </div>
  </div>
</div>
</div>


<div class="darker">
<div class="container">
<div>

  <div class="row pb-4 pl-4">

    <div class="col-12 px-0 papers-container papers-selected">
      <a name="people">
      <h4 class="paperhi pb-1">Lab Members</h4>
    </a></div><a name="people">

    </a><div class="col-2 col-lg-1 px-0 student pb-3"><a name="people">
      </a><a href="https://cheoljun95.github.io/"><img src="./Berkeley Speech Group_Files/cheoljun.jpg" class="img-fluid shadow-sm">
    </a></div><div class="col-4 col-lg-2 pr-2 student my-auto pb-3"><a href="https://cheoljun95.github.io/">
      Cheol Jun Cho</a><br><small>PhD Student (with <a href="https://psychology.berkeley.edu/people/jack-l-gallant">Jack Gallant</a>)</small>
    </div>


    <div class="col-2 col-lg-1 px-0 student pb-3">
      <a href="https://jlian2.github.io/"><img src="./Berkeley Speech Group_Files/jiachen.jpg" class="img-fluid shadow-sm">
    </a></div><div class="col-4 col-lg-2 pr-2 student my-auto pb-3"><a href="https://jlian2.github.io/">
      Jiachen Lian</a><br><small>PhD Student</small>
    </div>


    <div class="col-2 col-lg-1 px-0 student pb-3">
      <a href="https://www.kaylolittlejohn.com/"><img src="./Berkeley Speech Group_Files/kaylo.jpg" class="img-fluid shadow-sm">
    </a></div><div class="col-4 col-lg-2 pr-2 student my-auto pb-3"><a href="https://www.kaylolittlejohn.com/">
      Kaylo Littlejohn</a><br><small>PhD Student</small>
    </div>



    <div class="col-2 col-lg-1 px-0 student pb-3">
      <a href="https://peter.onrender.com/"><img src="./Berkeley Speech Group_Files/peter.jpg" class="img-fluid shadow-sm">
    </a></div><div class="col-4 col-lg-2 pr-2 student my-auto pb-3"><a href="https://peter.onrender.com/">
      Peter Wu</a><br><small>PhD Student</small>
    </div>

    <div class="col-2 col-lg-1 px-0 student pb-3">
      <a href="https://www.stat.berkeley.edu/~yugroup/people/Robbie.html"><img src="./Berkeley Speech Group_Files/robbie.jpg" class="img-fluid shadow-sm">
    </a></div><div class="col-4 col-lg-2 pr-2 student my-auto pb-3"><a href="https://www.stat.berkeley.edu/~yugroup/people/Robbie.html">
   Robbie Netzorg</a><br><small>PhD Student (with <a href="https://binyu.stat.berkeley.edu/">Bin Yu</a>)</small>
    </div>

    <div class="col-2 col-lg-1 px-0 student pb-3">
      <a href="https://tinglok.netlify.app/"><img src="./Berkeley Speech Group_Files/tingle.jpg" class="img-fluid shadow-sm">
    </a></div><div class="col-4 col-lg-2 pr-2 student my-auto pb-3"><a href="https://tinglok.netlify.app/">
      Tingle Li</a><br><small>PhD Student</small>
    </div>

    <div class="col-12 px-0 pb-1 papers-container papers-selected">
      <h4 class="paperhi">Alumni</h4>
      <p><a href="https://joshchartier.github.io/">Josh Chartier</a> (BioE PhD 2019, now Research Scientist at Meta Reality Labs)</p>
    </div>


  </div>
</div>
</div>
</div>

<div class="container">
<div>
  <div class="row">
    <div class="order-0 col-12 px-4 papers-container papers-selected">
      <a name="papers">
 
      <div class="paperhi paperhi-only">
        <h4>Selected Publications
        <button type="button" class="ml-3 btn btn-outline-info btn-sm shadow-sm"><a href="https://scholar.google.com/citations?user=VecEj6kAAAAJ&hl=en">Show All</a></button>
        </h4>
      </div>
        
          </a><div class="publication media paperhi"><a name="papers"></a>
          <p><strong><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/wu22i_interspeech.pdf">Deep Speech Synthesis from Articulatory Representations</a></strong> <span class="badge bg-danger text-light">New!</span><br>
          Peter Wu, Shinji Watanabe, Louis Goldstein, Alan Black, <b>Gopala Anumanchipalli</b><br>
          Interspeech, 2022<br>
          <span class="links"><a href="javascript:toggleblock('abstract-1')">Abstract</a> <a href="https://articulatorysynthesis.github.io/">Project Page</a> <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/wu22i_interspeech.pdf">Paper</a></span>
          <span id="abstract-1" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> In the articulatory synthesis task, speech is synthesized from input features containing information about the physical behavior of the human vocal tract. This task provides a promising direction for speech synthesis research, as the articulatory space is compact, smooth, and interpretable. Current works have highlighted the potential for deep learning models to perform articulatory synthesis. However, it remains unclear whether these models can achieve the efficiency and fidelity of the human speech production system. To help bridge this gap, we propose a time-domain articulatory synthesis methodology and demonstrate its efficacy with both electromagnetic articulography (EMA) and synthetic articulatory feature inputs. Our model is computationally efficient and achieves a transcription word error rate (WER) of 18.5% for the EMA-to-speech task, yielding an improvement of 11.6% compared to prior work. Through interpolation experiments, we also highlight the generalizability and interpretability of our approach.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/lian22_interspeech.pdf">Towards Improved Zero-shot Voice Conversion with Conditional DSVAE</a></strong> <span class="badge bg-danger text-light">New!</span><br>
          Jiachen Lian, Chunlei Zhang, <b>Gopala Anumanchipalli</b>, Dong Yu<br>
          Interspeech, 2022<br>
          <span class="links"><a href="javascript:toggleblock('abstract-2')">Abstract</a> <a href="https://jlian2.github.io/Improved-Voice-Conversion-with-Conditional-DSVAE/">Project Page</a> <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/lian22_interspeech.pdf">Paper</a></span> 
          <span id="abstract-2" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> Disentangling content and speaking style information is essential for zero-shot non-parallel voice conversion (VC). Our previous study investigated a novel framework with disentangled sequential variational autoencoder (DSVAE) as the backbone for information decomposition. We have demonstrated that simultaneous disentangling content embedding and speaker embedding from one utterance is feasible for zero-shot VC. In this study, we continue the direction by raising one concern about the prior distribution of content branch in the DSVAE baseline. We find the random initialized prior distribution will force the content embedding to reduce the phonetic-structure information during the learning process, which is not a desired property. Here, we seek to achieve a better content embedding with more phonetic information preserved. We propose conditional DSVAE, a new model that enables content bias as a condition to the prior modeling and reshapes the content embedding sampled from the posterior distribution. In our experiment on the VCTK dataset, we demonstrate that content embeddings derived from the conditional DSVAE overcome the randomness and achieve a much better phoneme classification accuracy, a stabilized vocalization and a better zero-shot VC performance compared with the competitive DSVAE baseline.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/lian22b_interspeech.pdf">Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition</a></strong> <span class="badge bg-danger text-light">New!</span><br>
          Jiachen Lian, Alan Black, Louis Goldstein, <b>Gopala Anumanchipalli</b><br>
          Interspeech, 2022<br>
          <span class="links"><a href="javascript:toggleblock('abstract-3')">Abstract</a> <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/lian22b_interspeech.pdf">Paper</a> <a href="https://github.com/Berkeley-Speech-Group/ema_gesture">Code</a></span> 
          <span id="abstract-3" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> Most of the research on data-driven speech representation learning has focused on raw audios in an end-to-end manner, paying little attention to their internal phonological or gestural structure. This work, investigating the speech representations derived from articulatory kinematics signals, uses a neural implementation of convolutive sparse matrix factorization to decompose the articulatory data into interpretable gestures and gestural scores. By applying sparse constraints, the gestural scores leverage the discrete combinatorial properties of phonological gestures. Phoneme recognition experiments were additionally performed to show that gestural scores indeed code phonological information successfully. The proposed work thus makes a bridge between articulatory phonology and deep neural networks to leverage informative, intelligible, interpretable,and efficient speech representations.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://arxiv.org/pdf/2206.02512.pdf">UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder</a></strong> <span class="badge bg-danger text-light">New!</span><br>
          Jiachen Lian, Chunlei Zhang, <b>Gopala Anumanchipalli</b>, Dong Yu<br>
          arXiv, 2022<br>
          <span class="links"><a href="javascript:toggleblock('abstract-4')">Abstract</a> <a href="https://neurtts.github.io/utts_demo/">Project Page</a> <a href="https://arxiv.org/pdf/2206.02512.pdf">Paper</a></span> 
          <span id="abstract-4" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> In this paper, we propose a novel unsupervised text-to-speech (UTTS) framework which does not require text-audio pairs for the TTS acoustic modeling (AM). UTTS is a multi-speaker speech synthesizer developed from the perspective of disentangled speech representation learning. The framework offers a flexible choice of a speaker's duration model, timbre feature (identity) and content for TTS inference. We leverage recent advancements in self-supervised speech representation learning as well as speech synthesis front-end techniques for the system development. Specifically, we utilize a lexicon to map input text to the phoneme sequence, which is expanded to the frame-level forced alignment (FA) with a speaker-dependent duration model. Then, we develop an alignment mapping module that converts the FA to the unsupervised alignment (UA). Finally, a Conditional Disentangled Sequential Variational Auto-encoder (C-DSVAE), serving as the self-supervised TTS AM, takes the predicted UA and a target speaker embedding to generate the mel spectrogram, which is ultimately converted to waveform with a neural vocoder. We show how our method enables speech synthesis without using a paired TTS corpus. Experiments demonstrate that UTTS can synthesize speech of high naturalness and intelligibility measured by human and objective evaluations.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.nejm.org/doi/pdf/10.1056/NEJMoa2027540?articleTools=true">Neuroprosthesis for Decoding Speech in a Paralyzed Person with Anarthria</a></strong><br>
          David A Moses, Sean Metzger, Jessie Liu, <b>Gopala Anumanchipalli</b>, et al.<br>
          New England Journal of Medicine, 2021<br>
          <span class="links"><a href="javascript:toggleblock('abstract-5')">Abstract</a> <a href="https://www.nejm.org/doi/pdf/10.1056/NEJMoa2027540?articleTools=true">Paper</a> <a href="https://www.ucsf.edu/news/2021/07/420946/neuroprosthesis-restores-words-man-paralysis">Press</a></span> 
          <span id="abstract-5" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> Technology to restore the ability to communicate in paralyzed persons who cannot speak has the potential to improve autonomy and quality of life. An approach that decodes words and sentences directly from the cerebral cortical activity of such patients may represent an advancement over existing methods for assisted communication.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://changlab.ucsf.edu/s/Sun_2020_JNeuralEng.pdf">Brain2Char: A Deep Architecture for Decoding Text from Brain Recordings</a></strong><br>
          Pengfei Sun, <b>Gopala Anumanchipalli</b>, Edward Chang<br>
          Journal of Neural Engineering, 2020<br>
          <span class="links"><a href="javascript:toggleblock('abstract-6')">Abstract</a> <a href="https://changlab.ucsf.edu/s/Sun_2020_JNeuralEng.pdf">Paper</a></span> 
          <span id="abstract-6" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> Decoding language representations directly from the brain can enable new brain–computer interfaces (BCIs) for high bandwidth human–human and human–machine communication. Clinically, such technologies can restore communication in people with neurological conditions affecting their ability to speak. In this study, we propose a novel deep network architecture Brain2Char, for directly decoding text (specifically character sequences) from direct brain recordings (called electrocorticography, ECoG). Brain2Char framework combines state-of-the-art deep learning modules—3D Inception layers for multiband spatiotemporal feature extraction from neural data and bidirectional recurrent layers, dilated convolution layers followed by language model weighted beam search to decode character sequences, and optimizing a connectionist temporal classification loss. Additionally, given the highly non-linear transformations that underlie the conversion of cortical function to character sequences, we perform regularizations on the network's latent representations motivated by insights into cortical encoding of speech production and artifactual aspects specific to ECoG data acquisition. To do this, we impose auxiliary losses on latent representations for articulatory movements, speech acoustics and session specific non-linearities. In three (out of four) participants reported here, Brain2Char achieves 10.6%, 8.5%, and 7.0% word error rates respectively on vocabulary sizes ranging from 1200 to 1900 words. Significance. These results establish a new end-to-end approach on decoding text from brain signals and demonstrate the potential of Brain2Char as a high-performance communication BCI.</span> 
          </p>
          </div>                  

          <div class="publication media paperhi">
          <p><strong><a href="https://jamanetwork.com/journals/jama/article-abstract/2758116">Toward a Speech Neuroprosthesis</a></strong><br>
          Edward Chang, <b>Gopala Anumanchipalli</b><br>
          Journal of the American Medical Association (JAMA), 2020<br>
          <span class="links"> <a href="javascript:toggleblock('abstract-7')">Abstract</a> <a href="https://jamanetwork.com/journals/jama/article-abstract/2758116">Paper</a></span> 
          <span id="abstract-7" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> Spoken communication is a basic human function. As such, loss of the ability to speak can be devastating for affected individuals. Stroke or neurodegenerative conditions, such as amyotrophic lateral sclerosis, can result in paralysis or dysfunction of vocal structures that produce speech. Current options are assistive devices that use residual movements, for example, cheek twitches or eye movements, to navigate alphabet displays to type out words.¹ While some users depend on these alternative communication approaches, these devices tend to be slow, error-prone, and laborious. A next generation of rehabilitative technologies currently being developed, called brain-computer interfaces (BCIs), directly read out brain signals to replace lost function. The application of neuroprostheses to restore speech has the potential to improve the quality of life of patients with neurological disease, but also including patients who have lost speech from vocal tract injury (e.g. from cancer or cancer-related surgery).</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.nature.com/articles/s41586-019-1119-1?TB_iframe=true&width=921.6&height=921.6">Speech Synthesis from Nural Decoding of Spoken Sentences</a></strong> <span class="badge bg-danger text-light">Hot!</span><br>
          <b>Gopala Anumanchipalli</b>, Josh Chartier, Edward Chang<br>
          Nature, 2019<br>
          <span class="links"><a href="javascript:toggleblock('abstract-8')">Abstract</a> <a href="https://www.nature.com/articles/s41586-019-1119-1?TB_iframe=true&width=921.6&height=921.6">Paper</a> <a href="https://www.youtube.com/watch?v=3pv0vT82Cys&ab_channel=UCSFNeurosurgery">Video</a> <a href="https://bionewscentral.com/speech-synthesis-from-neural-decoding-of-spoken-sentences/">Press</a></span> 
          <span id="abstract-8" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> Technology that translates neural activity into speech would be transformative for people who are unable to communicate as a result of neurological impairments. Decoding speech from neural activity is challenging because speaking requires very precise and rapid multi-dimensional control of vocal tract articulators. Here we designed a neural decoder that explicitly leverages kinematic and sound representations encoded in human cortical activity to synthesize audible speech. Recurrent neural networks first decoded directly recorded cortical activity into representations of articulatory movement, and then transformed these representations into speech acoustics. In closed vocabulary tests, listeners could readily identify and transcribe speech synthesized from cortical activity. Intermediate articulatory dynamics enhanced performance even with limited data. Decoded articulatory representations were highly conserved across speakers, enabling a component of the decoder to be transferrable across participants. Furthermore, the decoder could synthesize speech when a participant silently mimed sentences. These findings advance the clinical viability of using speech neuroprosthetic technology to restore spoken communication.</span> 
          </p>
          </div>          

          <div class="publication media paperhi">
          <p><strong><a href="https://www.sciencedirect.com/science/article/pii/S0896627318303398">Encoding of Articulatory Kinematic Trajectories in Human Speech Sensorimotor Cortex</a></strong><br>
          Josh Chartier, <b>Gopala Anumanchipalli</b>, Keith Johnson, Edward Chang<br>
          Neuron, 2018<br>
          <span class="links"><a href="javascript:toggleblock('abstract-9')">Abstract</a> <a href="https://www.sciencedirect.com/science/article/pii/S0896627318303398">Paper</a></span> 
          <span id="abstract-9" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> When speaking, we dynamically coordinate movements of our jaw, tongue, lips, and larynx. To investigate the neural mechanisms underlying articulation, we used direct cortical recordings from human sensorimotor cortex while participants spoke natural sentences that included sounds spanning the entire English phonetic inventory. We used deep neural networks to infer speakers' articulator movements from produced speech acoustics. Individual electrodes encoded a diversity of articulatory kinematic trajectories (AKTs), each revealing coordinated articulator movements toward specific vocal tract shapes. AKTs captured a wide range of movement types, yet they could be differentiated by the place of vocal tract constriction. Additionally, AKTs manifested out-and-back trajectories with harmonic oscillator dynamics. While AKTs were functionally stereotyped across different sentences, context-dependent encoding of preceding and following movements during production of the same phoneme demonstrated the cortical representation of coarticulation. Articulatory movements encoded in sensorimotor cortex give rise to the complex kinematics underlying continuous speech production.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://arxiv.org/pdf/2206.02512.pdf">Data-driven Intonational Phonology</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Alan Black, Luis Oliveira<br>
          Journal of The Acoustical Society of America, 2013<br>
          <span class="links"><a href="javascript:toggleblock('abstract-10')">Abstract</a> <a href="https://neurtts.github.io/utts_demo/">Project Page</a> <a href="https://arxiv.org/pdf/2206.02512.pdf">Paper</a></span> 
          <span id="abstract-10" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> Intonational Phonology deals with the systematic way in which speakers effectively use pitch to add appropriate emphasis to the underlying string of words in an utterance. Two widely discussed aspects of pitch are the pitch accents and boundary events. These provide an insight into the sentence type, speaker attitude, linguistic background, and other aspects of prosodic form. The main hurdle, however, is the difficulty in getting annotations of these attributes in “real” speech. Besides being language independent, these attributes are known to be subjective and prone to high inter-annotator disagreements. Our investigations aim to automatically derive phonological aspects of intonation from large speech databases. Recurring and salient patterns in the pitch contours, observed jointly with an underlying linguistic context are automatically detected. Our computational framework unifies complementary paradigms such as the physiological Fujisaki model, Autosegmental Metrical phonology, and elegant pitch stylization, to automatically (i) discover phonologically atomic units to describe the pitch contours and (ii) build inventories of tones and long term trends appropriate for the given speech database, either large multi-speaker or single speaker databases, such as audiobooks. We successfully demonstrate the framework in expressive speech synthesis. There is also immense potential for the approach in speaker, style, and language characterization.</span>
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.cs.cmu.edu/~gopalakr/publications/stylef0vc_icassp13.pdf">A Style Capturing Approach to F0 Transformation in Voice Conversion</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Luis Oliveira, Alan Black<br>
          ICASSP, 2013 <font color="red"><strong>(IEEE Spoken Language Processing Grant Award)</strong></font><br>
          <span class="links"><a href="javascript:toggleblock('abstract-11')">Abstract</a> <a href="https://www.cs.cmu.edu/~gopalakr/publications/stylef0vc_icassp13.pdf">Paper</a></span> 
          <span id="abstract-11" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> In this paper, we present a new approach to F0 transformation, that can capture aspects of speaking style. Instead of using the traditional 5ms frames as units in transformation, we propose a method that looks at longer phonological regions such as metrical feet. We automatically detect metrical feet in the source speech, and for each of source speaker’s feet, we find its phonological correspondence in target speech. We use a statistical phrase accent model to represent the F0 contour, where a 4-dimensional TILT representation is used for the F0 is parameterized over each feet region for the source and target speakers. This forms the parallel data that is the training data for our transformation. We transform the phrase component using simple z-score mapping. We use a joint density Gaussian mixture model to transform the accent contours. Our transformation method generates F0 contours that are significantly more correlated with the target speech than a baseline, frame-based method.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.cs.cmu.edu/~gopalakr/publications/agroupspss_icassp13.pdf">Accent Group modeling for Improved Prosody in Statistical Parameteric Speech Synthesis</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Luis Oliveira, Alan Black<br>
          ICASSP, 2013<br>
          <span class="links"><a href="javascript:toggleblock('abstract-12')">Abstract</a> <a href="https://www.cs.cmu.edu/~gopalakr/publications/agroupspss_icassp13.pdf">Paper</a></span> 
          <span id="abstract-12" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> This paper presents an ‘Accent Group’ based intonation model for statistical parametric speech synthesis. We propose an approach to automatically model phonetic realizations of fundamental frequency (F0) contours as a sequence of intonational events anchored to a group of syllables (an Accent Group). We train an accent grouping model specific to that of the speaker, using a stochastic context free grammar and contextual decision trees on the syllables. This model is used to ‘parse’ an unseen text into its constituent accent groups over each of which appropriate intonation is predicted. The performance of the model is shown objectively and subjectively on a variety of prosodically diverse tasks- read speech, news broadcast and audio books.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="http://www.cs.cmu.edu/afs/cs/Web/People/awb/papers/slt2012/slt2012_intent.pdf">Intent Transfer in Speech-to-Speech Machine Translation</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Luis Oliveira, Alan Black<br>
          SLT, 2012<br>
          <span class="links"><a href="javascript:toggleblock('abstract-13')">Abstract</a> <a href="http://www.cs.cmu.edu/afs/cs/Web/People/awb/papers/slt2012/slt2012_intent.pdf">Paper</a></span> 
          <span id="abstract-13" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> This paper presents an approach for transfer of speaker intent in speech-to-speech machine translation (S2SMT). Specifically, we describe techniques to retain the prominence patterns of the source language utterance through the translation pipeline and impose this information during speech synthesis in the target language. We first present an analysis of word focus across languages to motivate the problem of transfer. We then propose an approach for training an appropriate transfer function for intonation on a parallel speech corpus in the two languages within which the translation is carried out. We present our analysis and experiments on English↔Portuguese and English↔German language pairs and evaluate the proposed transformation techniques through objective measures.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.cs.cmu.edu/~gopalakr/publications/8341.pdf">Text-dependent Pathological Voice Detection</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Hugo Meinedo, Miguel Bugalho, Isabel Trancoso, Luis Oliveira, Alan Black<br>
          Interspeech, 2012<br>
          <span class="links"><a href="javascript:toggleblock('abstract-14')">Abstract</a> <a href="https://www.cs.cmu.edu/~gopalakr/publications/8341.pdf">Paper</a></span> 
          <span id="abstract-14" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> While global characteristics of the speaker’s source and spectral features have been successfully employed in pathological voice detection, the underlying text has largely been ignored. In this work, we focus on experiments that exploit the text stimulus that is read by the subject. Features derived from text include the mean cepstral distortion of the subject from an average intelligible speaker, and prosodic features include the speaking rate, statistics of phoneme durations, etc. The phonetic labeling information is also exploited to ignore all the unvoiced regions of the speech samples to improve the discriminability between intelligible and pathological voices. We also designed features that capture the speaker’s overall closeness to intelligible instances of the same text stimulus from other speakers. Our experiments show that the proposed text-derived features improve the detection of pathological voices by 20%.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.cs.cmu.edu/~gopalakr/publications/anumanchipalli_spamf0.PDF">A Statistical Phrase/Accent Model for Intonation Modeling</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Luis C. Oliveira, Alan Black<br>
          Interspeech, 2011<br>
          <span class="links"><a href="javascript:toggleblock('abstract-15')">Abstract</a> <a href="https://www.cs.cmu.edu/~gopalakr/publications/anumanchipalli_spamf0.PDF">Paper</a></span> 
          <span id="abstract-15" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> This paper proposes a statistical phrase/accent model of voice fundamental frequency(F0) for speech synthesis. It presents an approach for automatic extraction and modeling of phrase and accent phenomena from F0 contours by taking into account their overall trends in the training data. An iterative optimization algorithm is described to extract these components, minimizing the reconstruction error of the F0 contour. This method of modeling local and global components of F0 separately is shown to be better than conventional F0 models used in Statistical Parametric Speech Synthesis (SPSS). Perceptual evaluations confirm that the proposed model is significantly better than baseline SPSS F0 models in 3 prosodically diverse tasks – read speech, radio broadcast speech and audio book speech.</span> 
          </p>
          </div>

          <div class="publication media paperhi">
          <p><strong><a href="https://www.cs.cmu.edu/~gopalakr/publications/ctagging_icassp08.pdf">Significance of Early Tagged Contextual Graphemes in Grapheme Based Speech Synthesis and Recognition Systems</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Kishore Prahallad, Alan Black<br>
          ICASSP, 2008<br>
          <span class="links"><a href="javascript:toggleblock('abstract-16')">Abstract</a> <a href="https://www.cs.cmu.edu/~gopalakr/publications/ctagging_icassp08.pdf">Paper</a></span> 
          <span id="abstract-16" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> In this paper we present our argument that context information could be used in early stages i.e., during the definition of mapping of the words into sequence of graphemes. We show that the early tagged contextual graphemes play a significant role in improving the performance of grapheme based speech synthesis and speech recognition systems. </span> 
          </p>
          </div>                            

          <div class="publication media paperhi">
          <p><strong><a href="https://www.cs.cmu.edu/~gopalakr/publications/pinference_icassp07.pdf">Improving Pronunciation Inference using n-best list, Acoustics and Orthograhy</a></strong><br>
          <b>Gopala Anumanchipalli</b>, Mosur Ravishankar, Raj Reddy<br>
          ICASSP, 2007<br>
          <span class="links"><a href="javascript:toggleblock('abstract-17')">Abstract</a> <a href="https://www.cs.cmu.edu/~gopalakr/publications/pinference_icassp07.pdf">Paper</a></span> 
          <span id="abstract-17" style="white-space:normal; border:1px solid #cccccc; display:none; font-size:13px; padding:10px;"> In this paper, we tackle the problem of pronunciation inference and Out-of-Vocabulary (OOV) enrollment in Automatic Speech Recognition (ASR) applications. We combine linguistic and acoustic information of the OOV word using its spelling and a single instance of its utterance to derive an appropriate phonetic baseform. The novelty of the approach is in its employment of an orthography-driven n-best hypothesis and rescoring strategy of the pronunciation alternatives. We make use of decision trees and heuristic tree search to construct and score the n-best hypotheses space. We use acoustic alignment likelihood and phone transition cost to leverage the empirical evidence and phonotactic priors to rescore the hypotheses and refine the baseforms. /span> 
          </p>
          </div>                                                           




      </div>
      </div>
</div>
</div>

<div class="darker">
<div class="container">
<div>


  <div class="row">

    <div class="col-lg-6 col-12 px-4 order-1">

      <a name="teaching">

      <h4>Teaching</h4>
      </a><ul><a name="teaching">
        <li><a href="https://www2.eecs.berkeley.edu/Courses/EE225D/">EE225D: Audio Signal Processing in Humans and Machines, Fall 2022</a></li>
        <li><a href="https://www2.eecs.berkeley.edu/Courses/EE123/">EE123: Digital Signal Processing, Spring 2022</a></li>
        <li><a href="https://www2.eecs.berkeley.edu/Courses/EE225D/">EE225D: Audio Signal Processing in Humans and Machines, Fall 2021</a></li>
      </ul>                    
            
    </div>
    <div class="col-lg-6 col-12 px-4 order-1 pb-4">
           
    <a name="funding">
    <h4>Funding</h4>
      <ul>
        <li>National Science Foundation</li>
        <li>Rose Hills Innovator</li>
        <li>Google Research</li>
      </ul>

    </div>

  </div>

</div>
</div>
</div>




</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
